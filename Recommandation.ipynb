{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet NexFloox\n",
    "## Systeme de Recommandation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des librairies\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import psycopg2\n",
    "from dotenv import load_dotenv\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des variables d'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(r'variable_env/VAR.env')\n",
    "nom_base_de_donnees = os.getenv(\"NOM_BASES_DONNEES\")\n",
    "utilisateur = os.getenv(\"USERAZURE\")\n",
    "mot_de_passe = os.getenv(\"PASSWORD\")   \n",
    "host = os.getenv(\"HOST\")\n",
    "port = os.getenv(\"PORT\")\n",
    "path_donnees = os.getenv(\"PATH_DONNEES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connexion à la base de données (using sqlalchemy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connexion réussie à PostgreSQL avec SQLAlchemy !\n"
     ]
    }
   ],
   "source": [
    "# URL de connexion PostgreSQL\n",
    "DATABASE_URL = f\"postgresql://{utilisateur}:{mot_de_passe}@{host}:{port}/{nom_base_de_donnees}\"\n",
    "# Créer un moteur SQLAlchemy (engine)\n",
    "engine = create_engine(DATABASE_URL)\n",
    "\n",
    "# connexion à la BDD\n",
    "try:\n",
    "    with engine.connect() as connection:\n",
    "        print(\"Connexion réussie à PostgreSQL avec SQLAlchemy !\")\n",
    "except Exception as e:\n",
    "    print(\"Erreur de connexion :\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction des features interessantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ceci est l'exemple d'extraction sql vu avec Remi en cours\n",
    "\n",
    "SQL = '''\n",
    "WITH prep_ratings AS (\n",
    "\n",
    "SELECT tb_ratings.tconst AS id_film\n",
    ", \"averageRating\" AS target\n",
    ", \"numVotes\"\n",
    ", \"titleType\"\n",
    ", \"genres\"\n",
    ", split_part(\"genres\", ',', 1) AS genre_1\n",
    ", split_part(\"genres\", ',', 2) AS genre_2\n",
    ", split_part(\"genres\", ',', 3) AS genre_3\n",
    ", \"isAdult\"\n",
    ", \"startYear\"\n",
    "-- Données appartenant à la table title_crew\n",
    ", split_part(\"directors\", ',', 1) AS id_director_1\n",
    ", split_part(\"directors\", ',', 2) AS id_director_2\n",
    ", split_part(\"directors\", ',', 3) AS id_director_3\n",
    ", split_part(\"writers\", ',', 1) AS id_writer_1\n",
    "-- Données appartenant à la table name_basics\n",
    "\n",
    "FROM cyril_netfloox.\"title_ratings\" AS tb_ratings\n",
    "LEFT JOIN cyril_netfloox.\"title_basics\" AS tb_basics ON tb_ratings.tconst = tb_basics.tconst\n",
    "LEFT JOIN cyril_netfloox.\"title_crew\" AS tb_crew ON tb_ratings.tconst = tb_crew.tconst\n",
    "-- On veut faire correspondre les id_directors à la tables des noms de ces personnes, on va utiliser les CTE \n",
    "-- en debut de requete avec WITH on encapsule notre code prededant dans une table temporaire.\n",
    "--LEFT JOIN cyril_netfloox.\"name_basics\" AS tb_name ON tb_crew.tconst = tb_name.nconst\n",
    "\n",
    ")\n",
    "\n",
    "SELECT prep_ratings.*\n",
    ", tb_name_1.\"primaryName\" AS director_1\n",
    ", tb_name_2.\"primaryName\" AS director_2\n",
    ", tb_name_3.\"primaryName\" AS director_3\n",
    ", tb_name_4.\"primaryName\" AS writer_1\n",
    "FROM prep_ratings\n",
    "LEFT JOIN cyril_netfloox.\"name_basics\" AS tb_name_1 ON prep_ratings.id_director_1 = tb_name_1.nconst\n",
    "LEFT JOIN cyril_netfloox.\"name_basics\" AS tb_name_2 ON prep_ratings.id_director_2 = tb_name_2.nconst\n",
    "LEFT JOIN cyril_netfloox.\"name_basics\" AS tb_name_3 ON prep_ratings.id_director_3 = tb_name_3.nconst\n",
    "LEFT JOIN cyril_netfloox.\"name_basics\" AS tb_name_4 ON prep_ratings.id_writer_1 = tb_name_4.nconst;\n",
    "'''\n",
    "\n",
    "#df = pd.read_sql(SQL, engine)\n",
    "#print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on regarde les differentes catégories existantes pour ensuite créer les colonnes\n",
      "               category\n",
      "0                  self\n",
      "1                writer\n",
      "2                editor\n",
      "3              composer\n",
      "4      casting_director\n",
      "5   production_designer\n",
      "6       cinematographer\n",
      "7              director\n",
      "8       archive_footage\n",
      "9               actress\n",
      "10             producer\n",
      "11                actor\n"
     ]
    }
   ],
   "source": [
    "# !!!ATTENTION!!! ce test sera à réitérer sur la table entière\n",
    "# Mais il y a peu de chances qu'il y ait d'autres roles interessants\n",
    "\n",
    "SQL = '''\n",
    "SET search_path TO echantillon_test; -- A renouveller à chaque requête SQL\n",
    "SELECT DISTINCT category\n",
    "FROM title_principals\n",
    ";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(SQL, engine)\n",
    "connection.close()\n",
    "print(\"on regarde les differentes catégories existantes pour ensuite créer les colonnes\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on regarde les differentes régions existantes pour ensuite créer les colonnes\n",
      "   region\n",
      "0      BE\n",
      "1      CO\n",
      "2      DZ\n",
      "3      AZ\n",
      "4      RU\n",
      "..    ...\n",
      "86     TR\n",
      "87     CU\n",
      "88     AL\n",
      "89     HR\n",
      "90     GR\n",
      "\n",
      "[91 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SQL = '''\n",
    "SET search_path TO echantillon_test; -- A renouveller à chaque requête SQL\n",
    "SELECT DISTINCT region\n",
    "FROM title_akas\n",
    ";\n",
    "'''\n",
    "\n",
    "df = pd.read_sql(SQL, engine)\n",
    "connection.close()\n",
    "\n",
    "print(\"on regarde les differentes régions existantes pour ensuite créer les colonnes\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      titleId                                             region\n",
      "0   tt0000001                         DE, US, HU, GR, RU, UA, JP\n",
      "1   tt0000002                         HU, RO, DE, FR, US, RU, JP\n",
      "2   tt0000003                 GB, US, DE, FR, RO, HU, RU, UA, JP\n",
      "3   tt0000004                             HU, DE, FR, RO, RU, JP\n",
      "4   tt0000005         CA, US, HU, US, GB, US, US, DE, GB, UA, RU\n",
      "5   tt0000006                                 US, DE, PT, HU, RU\n",
      "6   tt0000007                 HU, US, DE, MX, US, US, RU, UA, JP\n",
      "7   tt0000008                 HU, US, US, DE, DE, US, RU, UA, JP\n",
      "8   tt0000009                                     AU, HU, US, DE\n",
      "9   tt0000010  AR, MX, CA, FR, BR, DE, US, US, IT, ES, FR, FR...\n",
      "10  tt0000011                                             DE, RU\n",
      "11  tt0000012  GB, US, XEU, XWW, XWW, CA, FR, BR, HU, DE, TR,...\n",
      "12  tt0000013  GB, US, XWW, HU, FR, ES, RS, FR, FR, FR, PL, U...\n",
      "13  tt0000014  HU, FR, DE, ES, DK, FR, IT, FR, FR, BR, PL, CZ...\n",
      "14  tt0000015                     ES, US, US, FR, HU, FR, RU, UA\n",
      "15  tt0000016    GB, US, XWW, FR, FI, RS, HU, FR, PL, FI, RU, UA\n",
      "16  tt0000017                                     DE, DE, RU, UA\n",
      "17  tt0000018                                     RO, DE, GB, RU\n",
      "18  tt0000019                                                 GB\n",
      "19  tt0000020                                             GB, RU\n"
     ]
    }
   ],
   "source": [
    "SQL='''\n",
    "SET search_path TO echantillon_test; -- A renouveller à chaque requête SQL\n",
    "SELECT \"titleId\"\n",
    ", STRING_AGG(region, ', ') AS region\n",
    "FROM title_akas\n",
    "GROUP BY \"titleId\"\n",
    "'''\n",
    "df = pd.read_sql(SQL, engine)\n",
    "connection.close()\n",
    "\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 18)\n",
      "      tconst            primaryTitle titleType  isAdult  startYear  \\\n",
      "0  tt0000001              Carmencita     short        0     1894.0   \n",
      "1  tt0000002  Le clown et ses chiens     short        0     1892.0   \n",
      "2  tt0000003            Poor Pierrot     short        0     1892.0   \n",
      "3  tt0000004             Un bon bock     short        0     1892.0   \n",
      "4  tt0000005        Blacksmith Scene     short        0     1893.0   \n",
      "\n",
      "                     genres  averageRating  numVotes  \\\n",
      "0         Documentary,Short            5.7    2121.0   \n",
      "1           Animation,Short            5.6     286.0   \n",
      "2  Animation,Comedy,Romance            6.4    2160.0   \n",
      "3           Animation,Short            5.3     183.0   \n",
      "4                     Short            6.2    2888.0   \n",
      "\n",
      "                             region       self writer     editor   composer  \\\n",
      "0              DE,US,HU,GR,RU,UA,JP  nm1588970   None       None       None   \n",
      "1              HU,RO,DE,FR,US,RU,JP       None   None       None  nm1335271   \n",
      "2        GB,US,DE,FR,RO,HU,RU,UA,JP       None   None  nm5442200  nm1335271   \n",
      "3                 HU,DE,FR,RO,RU,JP       None   None       None  nm1335271   \n",
      "4  CA,US,HU,US,GB,US,US,DE,GB,UA,RU       None   None       None       None   \n",
      "\n",
      "  cinematographer   director actress             producer                actor  \n",
      "0       nm0374658  nm0005690    None            nm0005690                 None  \n",
      "1            None  nm0721526    None                 None                 None  \n",
      "2            None  nm0721526    None  nm1770680,nm0721526                 None  \n",
      "3            None  nm0721526    None                 None                 None  \n",
      "4            None       None    None            nm0249379  nm0443482,nm0653042  \n"
     ]
    }
   ],
   "source": [
    "SQL = '''\n",
    "SET search_path TO echantillon_test; -- A renouveller à chaque requête SQL\n",
    "\n",
    "WITH tb_roles AS (\n",
    "--SET search_path TO echantillon_test; -- A renouveller à chaque requête SQL\n",
    "SELECT tconst\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'self') AS self\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'writer') AS writer\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'editor') AS editor\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'composer') AS composer\n",
    "--, STRING_AGG(nconst, ',') FILTER (WHERE category = 'casting_director') AS casting_director\n",
    "--, STRING_AGG(nconst, ',') FILTER (WHERE category = 'production_designer') AS production_designer\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'cinematographer') AS cinematographer\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'director') AS director\n",
    "--, STRING_AGG(nconst, ',') FILTER (WHERE category = 'archive_footage') AS archive_footage\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'actress') AS actress\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'producer') AS producer\n",
    ", STRING_AGG(nconst, ',') FILTER (WHERE category = 'actor') AS actor\n",
    "FROM title_principals\n",
    "GROUP BY tconst\n",
    "ORDER BY tconst\n",
    "),\n",
    "\n",
    "tb_regions AS (\n",
    "SELECT \"titleId\"\n",
    ", STRING_AGG(region, ',') AS region\n",
    "FROM title_akas\n",
    "GROUP BY \"titleId\"\n",
    ")\n",
    "\n",
    "SELECT tb_film.tconst\n",
    ", \"primaryTitle\"\n",
    ", \"titleType\"\n",
    ", \"isAdult\"\n",
    ", \"startYear\"\n",
    ", \"genres\"\n",
    "\n",
    "-- Dats from title_ratings\n",
    ", \"averageRating\"\n",
    ", \"numVotes\"\n",
    "\n",
    "-- Datas from  title_crew\n",
    "--, split_part(\"directors\", ',', 1) AS id_director_1\n",
    "--, split_part(\"directors\", ',', 2) AS id_director_2\n",
    "--, split_part(\"directors\", ',', 3) AS id_director_3\n",
    "--, split_part(\"writers\", ',', 1) AS id_writer_1\n",
    "--, split_part(\"writers\", ',', 2) AS id_writer_2\n",
    "--, split_part(\"writers\", ',', 3) AS id_writer_3\n",
    "-- Datas from tb_langages\n",
    "\n",
    "-- Datas from tb_regions\n",
    ", region\n",
    "\n",
    "-- Datas from tb_roles\n",
    ", self\n",
    ", writer\n",
    ", editor\n",
    ", composer\n",
    ", cinematographer\n",
    ", director\n",
    ", actress\n",
    ", producer\n",
    ", actor\n",
    "\n",
    "FROM title_basics AS tb_film\n",
    "LEFT JOIN title_ratings AS tb_notes ON tb_notes.tconst = tb_film.tconst\n",
    "LEFT JOIN title_crew AS tb_crew ON tb_crew.tconst = tb_film.tconst\n",
    "LEFT JOIN tb_regions ON tb_regions.\"titleId\" = tb_film.tconst\n",
    "LEFT JOIN tb_roles ON tb_film.tconst = tb_roles.tconst\n",
    ";\n",
    "'''\n",
    "df = pd.read_sql(SQL, engine)\n",
    "connection.close()\n",
    "\n",
    "\n",
    "print(df.shape)\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### On va nettoyer nos colonnes en remplacant les Nan par du texte vide, on met en minucuscule, puis on splitte en liste les colonnes contenant du texte séparé par des virgules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>titleType</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>genres</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>region</th>\n",
       "      <th>self</th>\n",
       "      <th>writer</th>\n",
       "      <th>editor</th>\n",
       "      <th>composer</th>\n",
       "      <th>cinematographer</th>\n",
       "      <th>director</th>\n",
       "      <th>actress</th>\n",
       "      <th>producer</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[documentary, short]</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>[de, us, hu, gr, ru, ua, jp]</td>\n",
       "      <td>[nm1588970]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0374658]</td>\n",
       "      <td>[nm0005690]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0005690]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.6</td>\n",
       "      <td>286.0</td>\n",
       "      <td>[hu, ro, de, fr, us, ru, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, comedy, romance]</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>[gb, us, de, fr, ro, hu, ru, ua, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm5442200]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1770680, nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.3</td>\n",
       "      <td>183.0</td>\n",
       "      <td>[hu, de, fr, ro, ru, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>[short]</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2888.0</td>\n",
       "      <td>[ca, us, hu, us, gb, us, us, de, gb, ua, ru]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0249379]</td>\n",
       "      <td>[nm0443482, nm0653042]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst            primaryTitle titleType  isAdult  startYear  \\\n",
       "0  tt0000001              Carmencita     short        0     1894.0   \n",
       "1  tt0000002  Le clown et ses chiens     short        0     1892.0   \n",
       "2  tt0000003            Poor Pierrot     short        0     1892.0   \n",
       "3  tt0000004             Un bon bock     short        0     1892.0   \n",
       "4  tt0000005        Blacksmith Scene     short        0     1893.0   \n",
       "\n",
       "                         genres  averageRating  numVotes  \\\n",
       "0          [documentary, short]            5.7    2121.0   \n",
       "1            [animation, short]            5.6     286.0   \n",
       "2  [animation, comedy, romance]            6.4    2160.0   \n",
       "3            [animation, short]            5.3     183.0   \n",
       "4                       [short]            6.2    2888.0   \n",
       "\n",
       "                                         region         self writer  \\\n",
       "0                  [de, us, hu, gr, ru, ua, jp]  [nm1588970]     []   \n",
       "1                  [hu, ro, de, fr, us, ru, jp]           []     []   \n",
       "2          [gb, us, de, fr, ro, hu, ru, ua, jp]           []     []   \n",
       "3                      [hu, de, fr, ro, ru, jp]           []     []   \n",
       "4  [ca, us, hu, us, gb, us, us, de, gb, ua, ru]           []     []   \n",
       "\n",
       "        editor     composer cinematographer     director actress  \\\n",
       "0           []           []     [nm0374658]  [nm0005690]      []   \n",
       "1           []  [nm1335271]              []  [nm0721526]      []   \n",
       "2  [nm5442200]  [nm1335271]              []  [nm0721526]      []   \n",
       "3           []  [nm1335271]              []  [nm0721526]      []   \n",
       "4           []           []              []           []      []   \n",
       "\n",
       "                 producer                   actor  \n",
       "0             [nm0005690]                      []  \n",
       "1                      []                      []  \n",
       "2  [nm1770680, nm0721526]                      []  \n",
       "3                      []                      []  \n",
       "4             [nm0249379]  [nm0443482, nm0653042]  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On stocke dans df_clean\n",
    "\n",
    "df_clean = df\n",
    "colonnes_string_to_list = ['genres','region','self','writer','editor','composer','cinematographer','director','actress','producer','actor']\n",
    "\n",
    "for col in colonnes_string_to_list:\n",
    "    \n",
    "    df_clean[col] = df[col].fillna(\"\")\n",
    "    df_clean[col] = df_clean[col].str.lower()\n",
    "    df_clean[col] = df_clean[col].apply(lambda x: x.split(\",\"))\n",
    "\n",
    "df[colonnes_string_to_list].head(20)\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>titleType</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>genres</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>region</th>\n",
       "      <th>self_1</th>\n",
       "      <th>self_2</th>\n",
       "      <th>self_3</th>\n",
       "      <th>self_4</th>\n",
       "      <th>self_5</th>\n",
       "      <th>self_6</th>\n",
       "      <th>self_7</th>\n",
       "      <th>self_8</th>\n",
       "      <th>self_9</th>\n",
       "      <th>self_10</th>\n",
       "      <th>writer_1</th>\n",
       "      <th>writer_2</th>\n",
       "      <th>writer_3</th>\n",
       "      <th>writer_4</th>\n",
       "      <th>editor_1</th>\n",
       "      <th>editor_2</th>\n",
       "      <th>editor_3</th>\n",
       "      <th>editor_4</th>\n",
       "      <th>composer_1</th>\n",
       "      <th>composer_2</th>\n",
       "      <th>composer_3</th>\n",
       "      <th>composer_4</th>\n",
       "      <th>cinematographer_1</th>\n",
       "      <th>cinematographer_2</th>\n",
       "      <th>cinematographer_3</th>\n",
       "      <th>cinematographer_4</th>\n",
       "      <th>director_1</th>\n",
       "      <th>director_2</th>\n",
       "      <th>director_3</th>\n",
       "      <th>director_4</th>\n",
       "      <th>actress_1</th>\n",
       "      <th>actress_2</th>\n",
       "      <th>actress_3</th>\n",
       "      <th>actress_4</th>\n",
       "      <th>actress_5</th>\n",
       "      <th>actress_6</th>\n",
       "      <th>actress_7</th>\n",
       "      <th>actress_8</th>\n",
       "      <th>actress_9</th>\n",
       "      <th>actress_10</th>\n",
       "      <th>producer_1</th>\n",
       "      <th>producer_2</th>\n",
       "      <th>producer_3</th>\n",
       "      <th>producer_4</th>\n",
       "      <th>actor_1</th>\n",
       "      <th>actor_2</th>\n",
       "      <th>actor_3</th>\n",
       "      <th>actor_4</th>\n",
       "      <th>actor_5</th>\n",
       "      <th>actor_6</th>\n",
       "      <th>actor_7</th>\n",
       "      <th>actor_8</th>\n",
       "      <th>actor_9</th>\n",
       "      <th>actor_10</th>\n",
       "      <th>actor_11</th>\n",
       "      <th>actor_12</th>\n",
       "      <th>actor_13</th>\n",
       "      <th>actor_14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[documentary, short]</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>[de, us, hu, gr, ru, ua, jp]</td>\n",
       "      <td>nm1588970</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0374658</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.6</td>\n",
       "      <td>286.0</td>\n",
       "      <td>[hu, ro, de, fr, us, ru, jp]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm1335271</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0721526</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, comedy, romance]</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>[gb, us, de, fr, ro, hu, ru, ua, jp]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm5442200</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm1335271</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0721526</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm1770680</td>\n",
       "      <td>nm0721526</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.3</td>\n",
       "      <td>183.0</td>\n",
       "      <td>[hu, de, fr, ro, ru, jp]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm1335271</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0721526</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>[short]</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2888.0</td>\n",
       "      <td>[ca, us, hu, us, gb, us, us, de, gb, ua, ru]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0249379</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0443482</td>\n",
       "      <td>nm0653042</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tt0000006</td>\n",
       "      <td>Chinese Opium Den</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[short]</td>\n",
       "      <td>5.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>[us, de, pt, hu, ru]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tt0000007</td>\n",
       "      <td>Corbett and Courtney Before the Kinetograph</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[short, sport]</td>\n",
       "      <td>5.3</td>\n",
       "      <td>898.0</td>\n",
       "      <td>[hu, us, de, mx, us, us, ru, ua, jp]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0374658</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td>nm0374658</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td>nm0249379</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0179163</td>\n",
       "      <td>nm0183947</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tt0000008</td>\n",
       "      <td>Edison Kinetoscopic Record of a Sneeze</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[documentary, short]</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>[hu, us, us, de, de, us, ru, ua, jp]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0374658</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0005690</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0653028</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tt0000009</td>\n",
       "      <td>Miss Jerry</td>\n",
       "      <td>movie</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[romance]</td>\n",
       "      <td>5.4</td>\n",
       "      <td>218.0</td>\n",
       "      <td>[au, hu, us, de]</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0085156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0085156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0085156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0063086</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0085156</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0183823</td>\n",
       "      <td>nm1309758</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tt0000010</td>\n",
       "      <td>Leaving the Factory</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1895.0</td>\n",
       "      <td>[documentary, short]</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7843.0</td>\n",
       "      <td>[ar, mx, ca, fr, br, de, us, us, it, es, fr, f...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0525910</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0525910</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>nm0525910</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst                                 primaryTitle titleType  isAdult  \\\n",
       "0  tt0000001                                   Carmencita     short        0   \n",
       "1  tt0000002                       Le clown et ses chiens     short        0   \n",
       "2  tt0000003                                 Poor Pierrot     short        0   \n",
       "3  tt0000004                                  Un bon bock     short        0   \n",
       "4  tt0000005                             Blacksmith Scene     short        0   \n",
       "5  tt0000006                            Chinese Opium Den     short        0   \n",
       "6  tt0000007  Corbett and Courtney Before the Kinetograph     short        0   \n",
       "7  tt0000008       Edison Kinetoscopic Record of a Sneeze     short        0   \n",
       "8  tt0000009                                   Miss Jerry     movie        0   \n",
       "9  tt0000010                          Leaving the Factory     short        0   \n",
       "\n",
       "   startYear                        genres  averageRating  numVotes  \\\n",
       "0     1894.0          [documentary, short]            5.7    2121.0   \n",
       "1     1892.0            [animation, short]            5.6     286.0   \n",
       "2     1892.0  [animation, comedy, romance]            6.4    2160.0   \n",
       "3     1892.0            [animation, short]            5.3     183.0   \n",
       "4     1893.0                       [short]            6.2    2888.0   \n",
       "5     1894.0                       [short]            5.0     204.0   \n",
       "6     1894.0                [short, sport]            5.3     898.0   \n",
       "7     1894.0          [documentary, short]            5.4    2270.0   \n",
       "8     1894.0                     [romance]            5.4     218.0   \n",
       "9     1895.0          [documentary, short]            6.8    7843.0   \n",
       "\n",
       "                                              region     self_1 self_2 self_3  \\\n",
       "0                       [de, us, hu, gr, ru, ua, jp]  nm1588970                 \n",
       "1                       [hu, ro, de, fr, us, ru, jp]                            \n",
       "2               [gb, us, de, fr, ro, hu, ru, ua, jp]                            \n",
       "3                           [hu, de, fr, ro, ru, jp]                            \n",
       "4       [ca, us, hu, us, gb, us, us, de, gb, ua, ru]                            \n",
       "5                               [us, de, pt, hu, ru]                            \n",
       "6               [hu, us, de, mx, us, us, ru, ua, jp]                            \n",
       "7               [hu, us, us, de, de, us, ru, ua, jp]                            \n",
       "8                                   [au, hu, us, de]                            \n",
       "9  [ar, mx, ca, fr, br, de, us, us, it, es, fr, f...                            \n",
       "\n",
       "  self_4 self_5 self_6 self_7 self_8 self_9 self_10   writer_1 writer_2  \\\n",
       "0                                                                         \n",
       "1                                                                         \n",
       "2                                                                         \n",
       "3                                                                         \n",
       "4                                                                         \n",
       "5                                                                         \n",
       "6                                                                         \n",
       "7                                                                         \n",
       "8                                                    nm0085156            \n",
       "9                                                                         \n",
       "\n",
       "  writer_3 writer_4   editor_1 editor_2 editor_3 editor_4 composer_1  \\\n",
       "0                                                                      \n",
       "1                                                          nm1335271   \n",
       "2                    nm5442200                             nm1335271   \n",
       "3                                                          nm1335271   \n",
       "4                                                                      \n",
       "5                                                                      \n",
       "6                                                                      \n",
       "7                                                                      \n",
       "8                                                                      \n",
       "9                                                                      \n",
       "\n",
       "  composer_2 composer_3 composer_4 cinematographer_1 cinematographer_2  \\\n",
       "0                                          nm0374658                     \n",
       "1                                                                        \n",
       "2                                                                        \n",
       "3                                                                        \n",
       "4                                                                        \n",
       "5                                                                        \n",
       "6                                          nm0374658                     \n",
       "7                                          nm0374658                     \n",
       "8                                          nm0085156                     \n",
       "9                                          nm0525910                     \n",
       "\n",
       "  cinematographer_3 cinematographer_4 director_1 director_2 director_3  \\\n",
       "0                                      nm0005690                         \n",
       "1                                      nm0721526                         \n",
       "2                                      nm0721526                         \n",
       "3                                      nm0721526                         \n",
       "4                                                                        \n",
       "5                                                                        \n",
       "6                                      nm0005690  nm0374658              \n",
       "7                                      nm0005690                         \n",
       "8                                      nm0085156                         \n",
       "9                                      nm0525910                         \n",
       "\n",
       "  director_4  actress_1 actress_2 actress_3 actress_4 actress_5 actress_6  \\\n",
       "0                                                                           \n",
       "1                                                                           \n",
       "2                                                                           \n",
       "3                                                                           \n",
       "4                                                                           \n",
       "5                                                                           \n",
       "6                                                                           \n",
       "7                                                                           \n",
       "8             nm0063086                                                     \n",
       "9                                                                           \n",
       "\n",
       "  actress_7 actress_8 actress_9 actress_10 producer_1 producer_2 producer_3  \\\n",
       "0                                           nm0005690                         \n",
       "1                                                                             \n",
       "2                                           nm1770680  nm0721526              \n",
       "3                                                                             \n",
       "4                                           nm0249379                         \n",
       "5                                                                             \n",
       "6                                           nm0005690  nm0249379              \n",
       "7                                           nm0005690                         \n",
       "8                                           nm0085156                         \n",
       "9                                           nm0525910                         \n",
       "\n",
       "  producer_4    actor_1    actor_2 actor_3 actor_4 actor_5 actor_6 actor_7  \\\n",
       "0                                                                            \n",
       "1                                                                            \n",
       "2                                                                            \n",
       "3                                                                            \n",
       "4             nm0443482  nm0653042                                           \n",
       "5                                                                            \n",
       "6             nm0179163  nm0183947                                           \n",
       "7             nm0653028                                                      \n",
       "8             nm0183823  nm1309758                                           \n",
       "9                                                                            \n",
       "\n",
       "  actor_8 actor_9 actor_10 actor_11 actor_12 actor_13 actor_14  \n",
       "0                                                               \n",
       "1                                                               \n",
       "2                                                               \n",
       "3                                                               \n",
       "4                                                               \n",
       "5                                                               \n",
       "6                                                               \n",
       "7                                                               \n",
       "8                                                               \n",
       "9                                                               "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On extraire les noms de personnes des listes et les mettre dans des colonnes \n",
    "# par role director_1, director_2, ... \n",
    "\n",
    "# # On stocke dans df_final\n",
    "\n",
    "colonnes_list_to_explode = ['self','writer','editor','composer','cinematographer','director','actress','producer','actor']\n",
    "df_final = df_clean[['tconst','primaryTitle','titleType','isAdult','startYear','genres','averageRating','numVotes','region']]\n",
    "for col in colonnes_list_to_explode:\n",
    "    \n",
    "    # Transformation avec explode() pour avoir une ligne par element de chaque liste de la colonne col\n",
    "    df_exploded = df_clean.explode(col)\n",
    "    # Ajout d'un index pour numéroter les colonnes à créer \n",
    "    df_exploded['index'] = df_exploded.groupby('tconst').cumcount() + 1\n",
    "    df_exploded['index'] = df_exploded['index'].fillna(-1)\n",
    "    df_exploded['index'] = df_exploded['index'].astype(int)\n",
    "    # Pivot pour transformer les lignes en colonnes\n",
    "    df_pivot = df_exploded.pivot_table(index=\"tconst\", columns=\"index\", values=col, aggfunc=\"first\")\n",
    "    # Renommer les colonnes\n",
    "    df_pivot.columns = [f\"{col}_{i}\" for i in df_pivot.columns]\n",
    "    # Réinitialiser l'index \n",
    "    df_pivot.reset_index(inplace=True)\n",
    "    #On supprime la col 'tconst' pour ne pas qu'elle soit dupliquer lors de la concaténation\n",
    "    df_pivot = df_pivot.drop('tconst', axis=1)\n",
    "    #On remplace les cellules Nan par du texte vide\n",
    "    df_pivot = df_pivot.fillna('')\n",
    "    #On concatène dans un nouveau df_final\n",
    "    df_final = pd.concat([df_final, df_pivot], axis=1)\n",
    "df_final.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMARQUE : Ici on a créé des colonnes roles contenant un id_name mais peut-être il peut être intéressant de créer des colonnes par couple nom-role. Cela crée beaucoup plus de colonnes mais je pense que l'on perd moins d'informations sur les personnes. Dans le cas actuel si un id_name apparait en tant que director et actor, le modèle ne 'voit pas' que c'est la même personne. \n",
    "\n",
    "# Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMARQUE  Après plusieurs essais, il semble compliqué de travailler sur 'df_final' avec des colonnes ainsi formatées.. Je vais revenir sur mon 'df_clean' et appliquer un mlb()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tconst</th>\n",
       "      <th>primaryTitle</th>\n",
       "      <th>titleType</th>\n",
       "      <th>isAdult</th>\n",
       "      <th>startYear</th>\n",
       "      <th>genres</th>\n",
       "      <th>averageRating</th>\n",
       "      <th>numVotes</th>\n",
       "      <th>region</th>\n",
       "      <th>self</th>\n",
       "      <th>writer</th>\n",
       "      <th>editor</th>\n",
       "      <th>composer</th>\n",
       "      <th>cinematographer</th>\n",
       "      <th>director</th>\n",
       "      <th>actress</th>\n",
       "      <th>producer</th>\n",
       "      <th>actor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1894.0</td>\n",
       "      <td>[documentary, short]</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2121.0</td>\n",
       "      <td>[de, us, hu, gr, ru, ua, jp]</td>\n",
       "      <td>[nm1588970]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0374658]</td>\n",
       "      <td>[nm0005690]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0005690]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.6</td>\n",
       "      <td>286.0</td>\n",
       "      <td>[hu, ro, de, fr, us, ru, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, comedy, romance]</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2160.0</td>\n",
       "      <td>[gb, us, de, fr, ro, hu, ru, ua, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm5442200]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1770680, nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1892.0</td>\n",
       "      <td>[animation, short]</td>\n",
       "      <td>5.3</td>\n",
       "      <td>183.0</td>\n",
       "      <td>[hu, de, fr, ro, ru, jp]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm1335271]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0721526]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>short</td>\n",
       "      <td>0</td>\n",
       "      <td>1893.0</td>\n",
       "      <td>[short]</td>\n",
       "      <td>6.2</td>\n",
       "      <td>2888.0</td>\n",
       "      <td>[ca, us, hu, us, gb, us, us, de, gb, ua, ru]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[nm0249379]</td>\n",
       "      <td>[nm0443482, nm0653042]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tconst            primaryTitle titleType  isAdult  startYear  \\\n",
       "0  tt0000001              Carmencita     short        0     1894.0   \n",
       "1  tt0000002  Le clown et ses chiens     short        0     1892.0   \n",
       "2  tt0000003            Poor Pierrot     short        0     1892.0   \n",
       "3  tt0000004             Un bon bock     short        0     1892.0   \n",
       "4  tt0000005        Blacksmith Scene     short        0     1893.0   \n",
       "\n",
       "                         genres  averageRating  numVotes  \\\n",
       "0          [documentary, short]            5.7    2121.0   \n",
       "1            [animation, short]            5.6     286.0   \n",
       "2  [animation, comedy, romance]            6.4    2160.0   \n",
       "3            [animation, short]            5.3     183.0   \n",
       "4                       [short]            6.2    2888.0   \n",
       "\n",
       "                                         region         self writer  \\\n",
       "0                  [de, us, hu, gr, ru, ua, jp]  [nm1588970]     []   \n",
       "1                  [hu, ro, de, fr, us, ru, jp]           []     []   \n",
       "2          [gb, us, de, fr, ro, hu, ru, ua, jp]           []     []   \n",
       "3                      [hu, de, fr, ro, ru, jp]           []     []   \n",
       "4  [ca, us, hu, us, gb, us, us, de, gb, ua, ru]           []     []   \n",
       "\n",
       "        editor     composer cinematographer     director actress  \\\n",
       "0           []           []     [nm0374658]  [nm0005690]      []   \n",
       "1           []  [nm1335271]              []  [nm0721526]      []   \n",
       "2  [nm5442200]  [nm1335271]              []  [nm0721526]      []   \n",
       "3           []  [nm1335271]              []  [nm0721526]      []   \n",
       "4           []           []              []           []      []   \n",
       "\n",
       "                 producer                   actor  \n",
       "0             [nm0005690]                      []  \n",
       "1                      []                      []  \n",
       "2  [nm1770680, nm0721526]                      []  \n",
       "3                      []                      []  \n",
       "4             [nm0249379]  [nm0443482, nm0653042]  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_target = 'tconst'\n",
    "col_titre_text = 'primaryTitle'\n",
    "col_num = ['isAdult', 'startYear', 'averageRating', 'numVotes']\n",
    "col_title_type = 'titleType'\n",
    "col_list_text = ['genres', 'region', 'self', 'writer', 'editor','composer', 'cinematographer', 'director', 'actress', 'producer','actor']\n",
    "#col_list_text = 'self'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code ci-dessous est inutile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100000, 30)\n",
      "(100000, 121)\n",
      "(100000, 362)\n",
      "(100000, 3581)\n",
      "(100000, 3732)\n",
      "(100000, 3912)\n",
      "(100000, 4701)\n",
      "(100000, 5995)\n",
      "(100000, 11089)\n",
      "(100000, 11672)\n",
      "(100000, 19404)\n",
      "reduced\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "countv = CountVectorizer()\n",
    "le = LabelEncoder()\n",
    "tfidfv = TfidfVectorizer()\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "\n",
    "#def myMLB(X):\n",
    "#    mlb = MultiLabelBinarizer()\n",
    "#    return mlb.fit_transform(X)\n",
    "#text_list_transformer = FunctionTransformer(myMLB, validate=False)\n",
    "\n",
    "\n",
    "text_list_transformer = Pipeline(steps = [\n",
    "    ('mlb', MultiLabelBinarizer())\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_list_transformer, col_list_text)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_final1 = pd.DataFrame()\n",
    "data_final1['label_encoded'] = le.fit_transform(df_clean['tconst'])\n",
    "\n",
    "\n",
    "# On applique un mlb a toutes les colonnes texte excepte le titre\n",
    "for col in col_list_text:\n",
    "    transformed_data = mlb.fit_transform(df_clean[col])\n",
    "    #reduced_data = svd.fit_transform(transformed_data)\n",
    "    data_final1 = pd.concat([data_final1, pd.DataFrame(transformed_data)], axis=1)\n",
    "    print(data_final1.shape)\n",
    "    \n",
    "#data_final1.columns = data_final1.columns.astype(str)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "#reduced_data_final = svd.fit_transform(data_final1)\n",
    "\n",
    "#print(data_final1.shape)   \n",
    "#print(data_final1.head())\n",
    "#print(\"--------\")\n",
    "print(\"reduced\")\n",
    "#print(reduced_data_final.shape)\n",
    "#print(reduced_data_final)\n",
    "\n",
    "# On travaille sur la colonne titre 'primaryTitle'\n",
    "# On essaie un countVectorizer() mais on pourra aussi essayer un tfidfVectorizer()\n",
    "#transformed_data = countv.fit_transform(df_clean[col_titre_text])\n",
    "#df_transformed_data = pd.DataFrame(transformed_data.toarray(), columns = countv.get_feature_names_out())\n",
    "#data_final = pd.concat([data_final, df_transformed_data], axis=1)\n",
    "#print(data_final.shape)\n",
    "#print(df_transformed_data.head())\n",
    "\n",
    "#transformed_data = le.fit_transform(df_clean[col_title_type])\n",
    "#df_transformed_data = pd.DataFrame(transformed_data)\n",
    "#data_final1 = pd.concat([data_final1, df_transformed_data], axis=1)\n",
    "#print(df_transformed_data)\n",
    "#stdscl = StandardScaler()\n",
    "#transformed_data = stdscl.fit_transform(df_clean[col_num])\n",
    "#data_final1 = pd.concat([data_final1, pd.DataFrame(transformed_data)], axis=1)\n",
    "#print(data_final.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La cellule suivante est commentée. Elle fait planter le noyau car la matrice résultante fait 100000 x 100000 ! Ayant fait mes premiers essais sur un dataset de 10000 lignes, ça passait mais là ... Ca coince !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Création de la matrice de similarité\\ncosine_sim = cosine_similarity(data_final1.fillna(0), data_final1.fillna(0))\\n\\n# Création d\\'un DataFrame pour la lisibilité\\ndf_similarity = pd.DataFrame(cosine_sim, index=df_clean[\"primaryTitle\"], columns=df_clean[\"primaryTitle\"])\\n\\n# Affichage de la matrice de similarité\\nprint(df_similarity)\\ndf_similarity[\\'Carmencita\\'].sort_values(ascending=False)\\n# Récupérer les classes des labels\\nmlb = MultiLabelBinarizer()\\nmlb.fit(df_clean[col_list_text])\\ncolumn_names = mlb.classes_\\n\\n# Convertir la sortie transformée en DataFrame pour affichage\\ntransformed_df = pd.DataFrame(transformed_data, columns=column_names)\\n\\n# Affichage du DataFrame transformé\\nprint(transformed_df)\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "'''\n",
    "# Création de la matrice de similarité\n",
    "cosine_sim = cosine_similarity(data_final1.fillna(0), data_final1.fillna(0))\n",
    "\n",
    "# Création d'un DataFrame pour la lisibilité\n",
    "df_similarity = pd.DataFrame(cosine_sim, index=df_clean[\"primaryTitle\"], columns=df_clean[\"primaryTitle\"])\n",
    "\n",
    "# Affichage de la matrice de similarité\n",
    "print(df_similarity)\n",
    "df_similarity['Carmencita'].sort_values(ascending=False)\n",
    "# Récupérer les classes des labels\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb.fit(df_clean[col_list_text])\n",
    "column_names = mlb.classes_\n",
    "\n",
    "# Convertir la sortie transformée en DataFrame pour affichage\n",
    "transformed_df = pd.DataFrame(transformed_data, columns=column_names)\n",
    "\n",
    "# Affichage du DataFrame transformé\n",
    "print(transformed_df)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Même problème d'où la cellule suivante qui est commentée !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pandas as pd\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.metrics.pairwise import cosine_similarity\\nfrom sklearn.preprocessing import Normalizer, StandardScaler\\nimport numpy as np\\n\\ndata = df[[\\'tconst\\', \\'primaryTitle\\', \\'genres\\', \\'averageRating\\', \\'numVotes\\', \\'startYear\\', \\'isAdult\\', \\'titleType\\']]\\n\\n# Créer une nouvelle colonne \\'features\\' qui combine les informations textuelles\\n#col_features = [\\'director\\', \\'producer\\', \\'actor\\', \\'actress\\', \\'primaryTitle\\', \\'genres\\']\\ncol_features = [\\'director\\', \\'producer\\', \\'actor\\', \\'actress\\', \\'genres\\']\\ndata.loc[:,\\'features\\'] = \"\"\\nfor col in col_features:\\n    data.loc[:, col] = df[col]\\n    data.loc[:, col] = data[col].fillna(\"\")\\n    data.loc[:, \\'features\\'] = data[\\'features\\'] + \",\" + data[col].apply(lambda x: \\' \\'.join(x if isinstance(x, list) else [str(x)])).fillna(\\'\\')\\n\\ndata.loc[:, \\'features\\'] = data[\\'features\\'].apply(lambda x: x.replace(\",\", \" \",))\\ndata.loc[:, \\'features\\'] = data[\\'features\\'].str.strip()\\n\\n# Ajouter des caractéristiques pour les rôles multiples\\n#data[\\'multi_role\\'] = data.apply(lambda row: \\' \\'.join([role for role in [row[\\'actor\\'], row[\\'director\\'], row[\\'producer\\']] if row[\\'actor\\'] in role]), axis=1)\\n\\n# A voir si on traite d\\'autres stopwords...\\n# Utiliser TF-IDF pour vectoriser les caractéristiques textuelles\\ntfidf = TfidfVectorizer(stop_words=\\'english\\')\\ntfidf_matrix = tfidf.fit_transform(data[\\'features\\'])\\n#print(tfidf_matrix)\\n\\n##  La normalisation ne sert pas ici\\n# Normaliser les caractéristiques textuelles\\n#normalizer = Normalizer()\\n#tfidf_matrix_normalized = normalizer.fit_transform(tfidf_matrix)\\n#print(tfidf_matrix_normalized)\\n\\ncol_numeric = [\\'averageRating\\', \\'numVotes\\']\\ndata[col_numeric] = data[col_numeric].fillna(-1)\\nscaler = StandardScaler()\\nnumeric_features = scaler.fit_transform(data[col_numeric])\\n\\n#numeric_features_normalized = normalizer.fit_transform(numeric_features)\\n#print(numeric_features_normalized)\\n# Combiner les caractéristiques textuelles et numériques\\n#combined_features = np.hstack((tfidf_matrix_normalized.toarray(), numeric_features))\\n\\n# Calculer la similarité cosinus entre les films\\n#cosine_sim = cosine_similarity(combined_features, combined_features)\\ncosine_sim = cosine_similarity(tfidf_matrix.toarray(), tfidf_matrix.toarray())\\n\\nprint(cosine_sim)\\n# Fonction pour obtenir les recommandations\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "# On essaie une autre approche  consistant a concatener en une seule colonne \n",
    "#les colonnes contenant du texte, chaque mot separe par un espace\n",
    "\n",
    "'''\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "data = df[['tconst', 'primaryTitle', 'genres', 'averageRating', 'numVotes', 'startYear', 'isAdult', 'titleType']]\n",
    "\n",
    "# Créer une nouvelle colonne 'features' qui combine les informations textuelles\n",
    "#col_features = ['director', 'producer', 'actor', 'actress', 'primaryTitle', 'genres']\n",
    "col_features = ['director', 'producer', 'actor', 'actress', 'genres']\n",
    "data.loc[:,'features'] = \"\"\n",
    "for col in col_features:\n",
    "    data.loc[:, col] = df[col]\n",
    "    data.loc[:, col] = data[col].fillna(\"\")\n",
    "    data.loc[:, 'features'] = data['features'] + \",\" + data[col].apply(lambda x: ' '.join(x if isinstance(x, list) else [str(x)])).fillna('')\n",
    "\n",
    "data.loc[:, 'features'] = data['features'].apply(lambda x: x.replace(\",\", \" \",))\n",
    "data.loc[:, 'features'] = data['features'].str.strip()\n",
    "\n",
    "# Ajouter des caractéristiques pour les rôles multiples\n",
    "#data['multi_role'] = data.apply(lambda row: ' '.join([role for role in [row['actor'], row['director'], row['producer']] if row['actor'] in role]), axis=1)\n",
    "\n",
    "# A voir si on traite d'autres stopwords...\n",
    "# Utiliser TF-IDF pour vectoriser les caractéristiques textuelles\n",
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = tfidf.fit_transform(data['features'])\n",
    "#print(tfidf_matrix)\n",
    "\n",
    "##  La normalisation ne sert pas ici\n",
    "# Normaliser les caractéristiques textuelles\n",
    "#normalizer = Normalizer()\n",
    "#tfidf_matrix_normalized = normalizer.fit_transform(tfidf_matrix)\n",
    "#print(tfidf_matrix_normalized)\n",
    "\n",
    "col_numeric = ['averageRating', 'numVotes']\n",
    "data[col_numeric] = data[col_numeric].fillna(-1)\n",
    "scaler = StandardScaler()\n",
    "numeric_features = scaler.fit_transform(data[col_numeric])\n",
    "\n",
    "#numeric_features_normalized = normalizer.fit_transform(numeric_features)\n",
    "#print(numeric_features_normalized)\n",
    "# Combiner les caractéristiques textuelles et numériques\n",
    "#combined_features = np.hstack((tfidf_matrix_normalized.toarray(), numeric_features))\n",
    "\n",
    "# Calculer la similarité cosinus entre les films\n",
    "#cosine_sim = cosine_similarity(combined_features, combined_features)\n",
    "cosine_sim = cosine_similarity(tfidf_matrix.toarray(), tfidf_matrix.toarray())\n",
    "\n",
    "print(cosine_sim)\n",
    "# Fonction pour obtenir les recommandations\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On revient a la premiere transfo\n",
    "... Mais j'ai toujorus le même probleme de Kernel qui plante\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import FunctionTransformer\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\nfrom sklearn.feature_extraction.text import CountVectorizer\\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\\nfrom sklearn.feature_extraction.text import TfidfVectorizer\\nfrom sklearn.decomposition import TruncatedSVD\\nfrom sklearn.decomposition import PCA\\n\\nmlb = MultiLabelBinarizer()\\ncountv = CountVectorizer()\\nle = LabelEncoder()\\ntfidfv = TfidfVectorizer()\\n#def myMLB(X):\\n#    mlb = MultiLabelBinarizer()\\n#    return mlb.fit_transform(X)\\n#text_list_transformer = FunctionTransformer(myMLB, validate=False)\\n\\n\\ntext_list_transformer = Pipeline(steps = [\\n    (\\'mlb\\', MultiLabelBinarizer())\\n])\\n\\n# Combine transformers\\npreprocessor = ColumnTransformer(\\n    transformers=[\\n        (\\'text\\', text_list_transformer, col_list_text)\\n    ])\\n\\n\\ndata_final = pd.DataFrame()\\nprint(data_final.shape)\\ndata_final[\\'label_encoded\\'] = le.fit_transform(df_clean[\\'tconst\\'])\\nprint(data_final.shape)\\n\\n# On applique un mlb a toutes les colonnes texte excepte le titre\\nfor col in col_list_text:\\n    transformed_data = mlb.fit_transform(df_clean[col])\\n    data_final = pd.concat([data_final, pd.DataFrame(transformed_data)], axis=1)\\n\\ndata_count = countv.fit_transform(data[\\'features\\'])\\n\\ndata_final.columns = data_final.columns.astype(str)\\nsvd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\\n#svd.fit(data_final)\\npca = PCA(n_components=2)\\n#pca.fit(data_final)\\n#reduced_data_final = svd.fit_transform(data_final)\\nprint(\"reduced\")\\nprint(reduced_data_final.shape)\\nprint(reduced_data_final)\\nprint(\"--------\")\\nprint(data_final.shape)   \\nprint(data_final.head())\\n\\n\\n# On travaille sur la colonne titre \\'primaryTitle\\'\\n# On essaie un countVectorizer() mais on pourra aussi essayer un tfidfVectorizer()\\n#transformed_data = countv.fit_transform(df_clean[col_titre_text])\\n#df_transformed_data = pd.DataFrame(transformed_data.toarray(), columns = countv.get_feature_names_out())\\n#data_final = pd.concat([data_final, df_transformed_data], axis=1)\\n#print(data_final.shape)\\n#print(df_transformed_data.head())\\n\\ntransformed_data = le.fit_transform(df_clean[col_title_type])\\ndf_transformed_data = pd.DataFrame(transformed_data)\\ndata_final = pd.concat([data_final, df_transformed_data], axis=1)\\nprint(df_transformed_data)\\nstdscl = StandardScaler()\\ntransformed_data = stdscl.fit_transform(df_clean[col_num])\\ndata_final = pd.concat([data_final, pd.DataFrame(transformed_data)], axis=1)\\nprint(data_final.shape)\\n\\ncosine_sim = cosine_similarity(data_count, data_count)\\n\\n# 4. Création d\\'un DataFrame pour la lisibilité\\ndf_similarity = pd.DataFrame(cosine_sim, index=df_clean[\"tconst\"], columns=df_clean[\"tconst\"])\\n\\n# Affichage de la matrice de similarité\\nprint(df_similarity)\\ndf_similarity[\\'tt0000001\\'].sort_values(ascending=False)\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "countv = CountVectorizer()\n",
    "le = LabelEncoder()\n",
    "tfidfv = TfidfVectorizer()\n",
    "#def myMLB(X):\n",
    "#    mlb = MultiLabelBinarizer()\n",
    "#    return mlb.fit_transform(X)\n",
    "#text_list_transformer = FunctionTransformer(myMLB, validate=False)\n",
    "\n",
    "\n",
    "text_list_transformer = Pipeline(steps = [\n",
    "    ('mlb', MultiLabelBinarizer())\n",
    "])\n",
    "\n",
    "# Combine transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('text', text_list_transformer, col_list_text)\n",
    "    ])\n",
    "\n",
    "\n",
    "data_final = pd.DataFrame()\n",
    "print(data_final.shape)\n",
    "data_final['label_encoded'] = le.fit_transform(df_clean['tconst'])\n",
    "print(data_final.shape)\n",
    "\n",
    "# On applique un mlb a toutes les colonnes texte excepte le titre\n",
    "for col in col_list_text:\n",
    "    transformed_data = mlb.fit_transform(df_clean[col])\n",
    "    data_final = pd.concat([data_final, pd.DataFrame(transformed_data)], axis=1)\n",
    "\n",
    "data_count = countv.fit_transform(data['features'])\n",
    "\n",
    "data_final.columns = data_final.columns.astype(str)\n",
    "svd = TruncatedSVD(n_components=5, n_iter=7, random_state=42)\n",
    "#svd.fit(data_final)\n",
    "pca = PCA(n_components=2)\n",
    "#pca.fit(data_final)\n",
    "#reduced_data_final = svd.fit_transform(data_final)\n",
    "print(\"reduced\")\n",
    "print(reduced_data_final.shape)\n",
    "print(reduced_data_final)\n",
    "print(\"--------\")\n",
    "print(data_final.shape)   \n",
    "print(data_final.head())\n",
    "\n",
    "\n",
    "# On travaille sur la colonne titre 'primaryTitle'\n",
    "# On essaie un countVectorizer() mais on pourra aussi essayer un tfidfVectorizer()\n",
    "#transformed_data = countv.fit_transform(df_clean[col_titre_text])\n",
    "#df_transformed_data = pd.DataFrame(transformed_data.toarray(), columns = countv.get_feature_names_out())\n",
    "#data_final = pd.concat([data_final, df_transformed_data], axis=1)\n",
    "#print(data_final.shape)\n",
    "#print(df_transformed_data.head())\n",
    "\n",
    "transformed_data = le.fit_transform(df_clean[col_title_type])\n",
    "df_transformed_data = pd.DataFrame(transformed_data)\n",
    "data_final = pd.concat([data_final, df_transformed_data], axis=1)\n",
    "print(df_transformed_data)\n",
    "stdscl = StandardScaler()\n",
    "transformed_data = stdscl.fit_transform(df_clean[col_num])\n",
    "data_final = pd.concat([data_final, pd.DataFrame(transformed_data)], axis=1)\n",
    "print(data_final.shape)\n",
    "\n",
    "cosine_sim = cosine_similarity(data_count, data_count)\n",
    "\n",
    "# 4. Création d'un DataFrame pour la lisibilité\n",
    "df_similarity = pd.DataFrame(cosine_sim, index=df_clean[\"tconst\"], columns=df_clean[\"tconst\"])\n",
    "\n",
    "# Affichage de la matrice de similarité\n",
    "print(df_similarity)\n",
    "df_similarity['tt0000001'].sort_values(ascending=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution qui fonctionne\n",
    "Voici ci dessous une solution qui fonctionne. Mon erreur était de calculer une matrice de similarité complète. J'ai essayé de réduire en utilisant des fonction qui ne garde que la partie supérieure ou inférieure d'une matrice symétrique mais rien n'y faisait. \n",
    "Un monitoring système montrait que c'était l'espace mémoire qui saturait.De nombreux essais ont été effectué pour libérer de la mémoire en effaçant les DataFrames inutiles, en forçant le garbage collector, ... \n",
    "\n",
    "La solution m'a été finalement été suggéré par Jonathan (Merci) la veille la soutenance et consiste non pas à passer en paramètre 2 fois la matrice comme je le faisais mais la matrice et le vecteur du film dont on désire chercher la similarité. (Elémentaire mon cher Watson)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:,'features'] = \"\"\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, col] = df[col]\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=col, inplace=True)\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, col] = df[col]\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=col, inplace=True)\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, col] = df[col]\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=col, inplace=True)\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.loc[:, col] = df[col]\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=col, inplace=True)\n",
      "/var/folders/lm/5jb7d18d2_gfyv8y4w2tsg4r0000gn/T/ipykernel_13493/904268893.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(columns=col, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommandations : \n",
      "        tconst                    primaryTitle titleType  isAdult  startYear  \\\n",
      "388  tt0000392               The Enchanted Cup     short        0     1903.0   \n",
      "216  tt0000219  Aladdin and the Wonderful Lamp     short        0     1899.0   \n",
      "193  tt0000196        Faust and Mephistopheles     short        0     1898.0   \n",
      "430  tt0000434                 Dorothy's Dream     short        0     1903.0   \n",
      "178  tt0000181                      Cinderella     short        0     1898.0   \n",
      "\n",
      "               genres  averageRating  numVotes        region self  \\\n",
      "388  [fantasy, short]            NaN       NaN          [gb]   []   \n",
      "216  [fantasy, short]            6.6      32.0          [gb]   []   \n",
      "193  [fantasy, short]            4.8      32.0          [gb]   []   \n",
      "430  [fantasy, short]            NaN       NaN      [ru, gb]   []   \n",
      "178  [fantasy, short]            6.6      62.0  [ve, gb, ru]   []   \n",
      "\n",
      "          writer editor composer cinematographer     director      actress  \\\n",
      "388           []     []       []              []  [nm0095816]           []   \n",
      "216           []     []       []              []  [nm0808310]           []   \n",
      "193  [nm0324473]     []       []              []  [nm0808310]           []   \n",
      "430           []     []       []              []  [nm0808310]  [nm2940350]   \n",
      "178  [nm0674518]     []       []              []  [nm0808310]  [nm0809419]   \n",
      "\n",
      "        producer actor  \n",
      "388  [nm0666972]    []  \n",
      "216  [nm0808310]    []  \n",
      "193  [nm0808310]    []  \n",
      "430  [nm0808310]    []  \n",
      "178  [nm0808310]    []  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "#from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "#from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import fct_extract as my_fct\n",
    "import gc\n",
    "\n",
    "#df = my_fct.extraire_donnees_BDD()\n",
    "\n",
    "#data = df[['tconst', 'primaryTitle', 'genres', 'averageRating', 'numVotes', 'startYear', 'isAdult', 'titleType']]\n",
    "data = df[['primaryTitle', 'genres', 'averageRating', 'isAdult', 'titleType']]\n",
    "\n",
    "# Créer une nouvelle colonne 'features' qui combine les informations textuelles\n",
    "#col_features = ['director', 'producer', 'actor', 'actress', 'primaryTitle', 'genres']\n",
    "col_features = ['director', 'producer', 'actor', 'actress', 'genres']\n",
    "data.loc[:,'features'] = \"\"\n",
    "for col in col_features:\n",
    "    data.loc[:, col] = df[col]\n",
    "    data.loc[:, col] = data[col].fillna(\"\")\n",
    "    data.loc[:, 'features'] = data['features'] + \",\" + data[col].apply(lambda x: ' '.join(x if isinstance(x, list) else [str(x)])).fillna('')\n",
    "    # Je rajoute l'effacement de la colonne traitée\n",
    "    data.drop(columns=col, inplace=True)\n",
    "data.loc[:, 'features'] = data['features'].apply(lambda x: x.replace(\",\", \" \",))\n",
    "data.loc[:, 'features'] = data['features'].str.strip()\n",
    "\n",
    "def get_index_from_title(df, title):\n",
    "  # simple exemple a adapter\n",
    "  #print(df[df['primaryTitle'].fillna('').str.contains(title)].index)\n",
    " \n",
    "  return df[df['primaryTitle'].fillna('').str.contains(title)]\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "svd = TruncatedSVD(n_components=10)  # Ajustez le nombre de composants selon vos besoins\n",
    "pipeline = make_pipeline(vectorizer, svd)\n",
    "\n",
    "# Transformation des caractéristiques textuelles\n",
    "features_transformed = pipeline.fit_transform(data['features'])\n",
    "# On passe de float64 a float16\n",
    "features_transformed = features_transformed.astype(np.float16)\n",
    "\n",
    "# On sauvegarde la matrice de transformation\n",
    "file = 'features_transformed.npy'\n",
    "np.save(file, features_transformed)\n",
    "\n",
    "# On choisit un titre\n",
    "title = 'Enchanted Cup'\n",
    "movie_idx = get_index_from_title(df, title).index[0] # Index[0] permet de choisir le premier film qui sort\n",
    "\n",
    "X = np.load(file)\n",
    "Y = features_transformed[movie_idx, :]\n",
    "\n",
    "# calcule des similarités\n",
    "cosine_sim = cosine_similarity(X, Y.reshape(1,-1))\n",
    "\n",
    "# On récupère le n films les plus semblables\n",
    "# Pas encore tout compris mais ca marche ! ;-) \n",
    "n=5\n",
    "cosine_sim_1d = cosine_sim.flatten()\n",
    "idxs = np.argpartition(cosine_sim_1d, -n)[-n:]\n",
    "idxs = idxs[np.argsort(cosine_sim_1d[idxs])][::-1]\n",
    "\n",
    "# affichage des recommandations\n",
    "print('Recommandations : ')\n",
    "print(df.loc[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et bien c'est pas terrible ! Tout d'abord il est difficle de juger des films similaires proposé car on ne connait pas les films. Cependant, ici, en comparant les features de notre film avec les autres on voit que les seuls similarités sont surtout au niveau du genre. Je n'ai pas eu el temsp d'ajoueter d'autres features comme je l'avais fait orinelelement avec un dataset plus petit car j'ai ensuite tenté de récupérer des données plus récentes de la database complète qui contient 18 Millions de films mais je n'ai pas réussi. J'ai essayé de récupérer des films plus récents... en vain. J'ai essayé de créer une vue matérialisé ... en vain.\n",
    "J'aurais ensuite voulu tester un KNN Classifier mais là aussi pas eu le temps.\n",
    "\n",
    "Bref mon système de recommandation est plutôt un échec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
